{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Functions - Exact Frank Wolfe\n",
    "This code is to test functions in the `fw_fun.jl` file that we will use for *exact* Frank Wolfe algorithm. We are going to run on one small example, only to see that the code should be correct (bug-free). We will also spend a little bit of time to demonstrate why the code is written as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "includet(\"fw_fun.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Vertex Cover Gradient Oracle\n",
    "\n",
    "First, we'll check that the gradient oracle for the undirected weighted vertex cover is correct. Let's first review do this by re-deriving the gradient itself. We'll actually do this for the more general weighted set cover function.\n",
    "\n",
    "Let $\\mathcal{U} = \\{u_1, \\dots u_m \\}$ be a universe of $m$ elements, each with a non-negative weight $w_i$ for $i=1, \\dots , m$. Let $\\Omega = \\{ S_1, \\dots S_n \\}$ be a collection of subsets $S_i \\subset \\mathcal{U}$. For a collection \n",
    "of subsets, $X \\subset \\Omega$, define the *weighted set coverage function* to be \n",
    "$$f(X) = w \\left( \\cup_{S_i \\in X} S_i \\right) \\enspace.$$\n",
    "The multilinear extension $F: [0,1]^n \\rightarrow \\mathbb{R}$ is given by \n",
    "$$F(x) = \\sum_{j=1}^m w_j \\left( 1 - \\prod_{i : u_j \\in S_i} (1 - x_i) \\right) \\enspace. $$\n",
    "Let's compute the gradient. We'll do this by computing the coordiantes of the gradient, i.e. the partial derivative $\\frac{\\partial F(x)}{\\partial x_k}$ for each coordinate $k=1, \\dots n$. Now, by linearity, computing this partial derivative only requires observing that\n",
    "$$ \\frac{\\partial }{\\partial x_k} \\left[ 1 - \\prod_{i : u_j \\in S_i} (1 - x_i) \\right]\n",
    "= \\left\\{\n",
    "\\begin{array}{lr}\n",
    "       \\prod_{\\substack{i : u_j \\in S_i \\\\ i \\neq k}} (1 - x_i) & u_j \\in S_k\\\\\n",
    "       0 & u_j \\notin S_k                     \n",
    "     \\end{array}\n",
    "\\right.$$\n",
    "Now, multiplying by $w_j$ and summing these partial derivatives up over $j=1 \\dots m$ gives us the partial derviative $\\frac{\\partial F(x)}{\\partial x_k}$ that we were interested in.\n",
    "\n",
    "We are particularly interested in the case of vertex cover, where both the universe and the sets corespond to vertices of a graph and we saw that a vertex \"covers\" the neighbors that it points to.\n",
    "Once can check that the function `vertex_cover_grad()` takes as input an adjacency list and (optional) vector of weights and creates a function which returns the exact gradient. It is efficient because it uses the adjacency list to reduce a naive $O(n^3)$ computation to something which looks more like $O(n d^2)$ where $d$ is the average degree, which can be much smaller than $n$.\n",
    "\n",
    "\n",
    "Let's stary by creating a simple graph, say, a 4-cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64,Array{Int64,1}} with 4 entries:\n",
       "  4 => [3, 1]\n",
       "  2 => [1, 3]\n",
       "  3 => [2, 4]\n",
       "  1 => [4, 2]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_dict = Dict(1 => [4,2], 2 => [1,3], 3 => [2,4], 4 => [3,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll instantiate a gradient oracle without weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f_grad (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_grad = vertex_cover_grad(4, adj_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it in a couple of places. First, we know that $\\nabla F(0) = 3 \\mathbf{1}$ because $f(\\{e\\}) = 3$ for every vertex $e$. Additionally, we should get that $\\nabla F(\\mathbf{1}) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0\n",
       " 3.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = zeros(4)\n",
    "f_grad(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ones(4)\n",
    "f_grad(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 1.89\n",
       " 1.89\n",
       " 1.89\n",
       " 1.89"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_grad(0.3 * ones(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks correct. Now let's see whether using the `weights` arguement behaves well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f_grad (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = [1,2,3,4]\n",
    "f_grad_w = vertex_cover_grad(4, adj_dict, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 7.0\n",
       " 6.0\n",
       " 9.0\n",
       " 8.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_grad_w(zeros(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_grad(ones(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, I think that this looks correct. If we wanted to be *extra* sure, we could explicitly compute the gradient $\\nabla F(x)$ by hand for a given value of $x$ and then check that this matches what the algorithm returns. I really only checked two corner cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Uniform Matroid - Linear Optimization Oracle\n",
    "This one should be easy. Let's just check it in a few cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mat_lin_opt (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 3\n",
    "mat_lin_opt = uniform_mat_lin_opt(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 0.26011109480357897\n",
       " 0.6687946207031705 \n",
       " 0.8046414018826553 \n",
       " 0.501777201585758  \n",
       " 0.6580127701406866 "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = rand(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set([2, 3, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = mat_lin_opt(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set([4, 3, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [1,2,3,4,5]\n",
    "S = mat_lin_opt(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, this looks correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Partition Matroid - Linear Optimization Oracle\n",
    "You should recall the definition of partition matroid and convince yourself that the greedy algorithm for linear optimization can work with each partition independently. Then, observe that this is what my code does.\n",
    "\n",
    "Now let's construct a partition matroid which is relatively simple. We will use the sample ground set (the vertices of the 4-cycle) and our partition will be $P_1 = \\{1,2\\}$ and $P_2 = \\{3,4\\}$ where we allow $1$ element from $P_1$ and $2$ elements from $P_2$. Let's create the corresponding partition dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64,Tuple{Int64,Array{Int64,1}}} with 2 entries:\n",
       "  2 => (2, [3, 4])\n",
       "  1 => (1, [1, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_dict = Dict(1 => (1, [1,2]), 2 => (2, [3,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now creating the linear optimization oracle,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::getfield(Main, Symbol(\"#mat_lin_opt#10\")){Dict{Int64,Tuple{Int64,Array{Int64,1}}}}) (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part_mat_lin_opt = partition_mat_lin_opt(partition_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and choosing a linear function to optimize, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.9440308915213715\n",
       " 0.647167988450591 \n",
       " 0.9548338710311961\n",
       " 0.3291918519288237"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = rand(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set([4, 3, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = part_mat_lin_opt(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, that looks like it's working, let's try on more example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Int64,1}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Set([4, 2, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = part_mat_lin_opt(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, I believe that it is working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Set to Vector\n",
    "This test should be easy, because this function is easy. Let's try one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Int64,1}:\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = Set([1,2])\n",
    "v = set_to_vec(S, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Exact Frank Wolfe Algorithm\n",
    "Because $n=4$ is small, we can find a solution to the constrained problem very easily. This is just a test to see if my code actually *works*, i.e. it doesn't throw any bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, 0.0, 1.0, 1.0], Any[Set([4, 3, 1]), Set([4, 3, 1]), Set([4, 3, 1]), Set([4, 3, 1]), Set([4, 3, 1]), Set([4, 3, 1]), Set([4, 3, 1]), Set([4, 3, 1]), Set([4, 3, 1]), Set([4, 3, 1])])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem\n",
    "n = 4\n",
    "adj_dict = Dict(1 => [4,2], 2 => [1,3], 3 => [2,4], 4 => [3,1]) # specify the graph\n",
    "partition_dict = Dict(1 => (1, [1,2]), 2 => (2, [3,4])) # specify the partition\n",
    "\n",
    "# construct the oracles\n",
    "f_grad = vertex_cover_grad(n, adj_dict) # the gradient oracle for unweighted vertex cover\n",
    "mat_lin_opt = partition_mat_lin_opt(partition_dict) # the linear optimzation oracle for partition matroid\n",
    "\n",
    "# run frank wolfe\n",
    "T = 10 # number of iterations (inverse step size)\n",
    "x, v_set = fw_exact(n, T, f_grad, mat_lin_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, everything seemed to work fine. Plus, this is a small enough example that we can actually verify what the algorithm will do at every step. Here, I believe that picking the same base every time was what it should have done, although we might want to analytically verify this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "There are several next steps.\n",
    "\n",
    "1. **More Oracles** Implement more linear optimization oracles for specific matroids and set functions where the multilinear extension admits a closed-form solution. e.g. graphic matroids, realizable matroids\n",
    "2. **Rounding Schemes** Implement swap rounding algorithm of Chekuri et al for several of our favorite matroids. Note that we can work directly with the bases returned in `v_set`, rather than x. In fact, we could even do the swaps at each iteration of Frank Wolfe, but let's save that for later.\n",
    "3. **Stochastic FW Variants** There are several stochastic Frank Wolfe variants that we would like to implement. This requires coding (1) stochastic gradient oracle functions (2) "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
